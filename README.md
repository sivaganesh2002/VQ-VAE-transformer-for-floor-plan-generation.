This repository implements a discrete latent generative modeling pipeline for floor plan synthesis using a Vector Quantized Variational Autoencoder (VQ-VAE) followed by an autoregressive Transformer operating over learned latent tokens.

The VQ-VAE consists of a fully convolutional encoder–quantizer–decoder architecture trained on resized 64×64 RGB floor plan images. The encoder maps input images to a spatial latent tensor using stacked Conv2D + ReLU layers. A vector quantization module discretizes these continuous latents by nearest-neighbor lookup against a learnable codebook of embeddings, yielding a grid of discrete latent indices. Training is stabilized via a commitment loss, codebook loss, and a straight-through estimator (STE) to preserve gradient flow through the non-differentiable quantization operation. Reconstruction is performed using a transposed convolutional decoder, optimized jointly with MSE reconstruction loss and VQ loss.

After convergence, the learned discrete latent indices are flattened into token sequences and modeled using a decoder-only Transformer trained with causal autoregressive cross-entropy loss. This stage captures global spatial dependencies, long-range room connectivity, and layout regularities that convolutional decoders alone cannot model.

During inference, the Transformer generates novel latent token sequences, which are reshaped into latent grids, mapped back to embeddings using the VQ codebook, and decoded to produce coherent and structurally consistent floor plan images.

Below are sample reconstructions and generations observed during VQ-VAE training across epochs, illustrating progressive structural stabilization and layout emergence.

<img width="850" height="385" alt="image" src="https://github.com/user-attachments/assets/d3d261c9-0ae4-4547-b953-4d06d29e5e65" />
<img width="770" height="396" alt="image" src="https://github.com/user-attachments/assets/44338704-e956-4977-a4a8-20520fd9afdc" />

The following figure shows a **floor plan generated by the trained VQ-VAE + autoregressive Transformer**, conditioned on a structured natural-language spatial prompt.

**Input Prompt**

> Design a house floor plan with an east-facing living room at the center.  
> Kitchen is in the south middle between living room and bathroom.  
> Bathroom (10×6 ft) is in the east–south middle corner between kitchen and living room.  
> Common Room 1 (10×10 ft) is in the west–north middle between living room and Common Room 2.  
> Common Room 2 (10×10 ft) is at the north center between Common Room 1 and the master bedroom.  
> Master bedroom (10×14 ft) is in the north–east middle between balcony and storage.  
> Balcony (10×6 ft) is at the north corner near Common Room 2 and master bedroom.  
> Storage room (10×8 ft) is at the east corner between master bedroom and living room.

The model converts the spatial constraints described in the prompt into a **discrete latent token sequence**, which is then decoded into a coherent 2D layout using the learned VQ-VAE decoder. The generated result demonstrates **global spatial consistency**, **correct room adjacency**, and **orientation-aware placement** learned purely from data.

<img width="608" height="467" alt="image" src="https://github.com/user-attachments/assets/bc6c7bf9-95a3-4907-a347-c6914c39e50b" />


